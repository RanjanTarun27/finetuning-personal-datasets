# finetuning-personal-datasets
# ðŸš€ Fine-Tuning LLMs with LoRA (4-bit, Personal Dataset)

This repository provides a clean and memory-efficient setup for fine-tuning large language models (LLMs) like **LLaMA 2** using **LoRA** with **4-bit quantization** via `bitsandbytes`. Perfect for low-GPU environments like Colab and small-scale training.

---

## ðŸ“¦ Features

- âœ… Supports 4-bit and 8-bit model loading (via `bitsandbytes`)
- âœ… Hugging Face `transformers` and `datasets` integration
- âœ… PEFT-based LoRA fine-tuning (`peft`)
- âœ… Works on Google Colab (T4 / A100) or local GPUs
- âœ… Uses personal instruction datasets (`instruction`, `input`, `output` format)



